# deep voice论文学习
## 系统结构

![](https://github.com/sysu16340234/deep_voice_learning/blob/master/imgs/deep_voice.png)

系统包含五个基本模块:**字素转换音素模型,分割模型,音素持续时间模型,基频模型,音频合成模型**:

* **字素转换音素模型**:用来将文本转化为音素
* **分割模型**:分割模型用来识别给定音频中每个音素的边界,即开始和结束的位置
* **音素持续时间模型**:用来预测音素序列中每个音素的持续时间
* **基频模型**:预测音素是否为浊音,如果是浊音，则在在音素的持续时间过程中预测基频;
* **音频合成模型**:组合了字素转换音素模型,音素持续时间模型,基频模型,以高采样率生成所需文本的音频;

**训练过程**:给定音频和对应文本,将文本通过G2P模型转换为音素,将音素和音频输入分割模块,使用分割模块将音素的边界作标记即得到音素持续时间,将音素持续时间,音素和音频输入音频合成模型作为训练样本,将持续时间和音素输入持续时间模型作为训练样本,对音频进行基频提取,然后将基频和音素输入基频模型作为训练样本;

**推理过程**:文本经过字素转换音素模型或者使用音素字典转换成音素,再把音素提供给音素持续时间模型和基频模型预测每个音素的持续时间和基频,最后将基频和音素持续时间作为局部条件特征输入音频合成模型,音频合成模型使用这两类特征和音素合成音频;

## 模型细节
**1.字素转换音素模型**

基于[(Yao & Zweig, 2015)](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnnlts.pdf)的编码器和解码器结构,使用具有非线性GRU的多层双向编码器和同样深度的单层GRU解码器;使用teacher forcing,解码采用beam search,使用三层双向1024个单元的解码器和三层单元数相同的解码器,beam search的宽度为5,对于每层采用概率为0.95的dropout.采用Adam优化算法,参数设置:β1 = 0.9, β2 = 0.999, ε = 10^−8,批大小为64,学习率为10^-3,每1000次迭代执行概率为0.85的退火.一个双向LSTMencoder-decoder的例图如下(虚线左边为编码器,右边为解码器):

![encoder_decoder](https://github.com/sysu16340234/deep_voice_learning/blob/master/imgs/encoder_decoder.png)

**2.分割模型**

基于[(Amodei et al., 2015)](https://arxiv.org/pdf/1512.02595.pdf)的卷积递归神经网络实现,在CTC的基础上做出改进:将相邻的两个音素对的边界作为结果输出.通过以10ms为步长计算20个mel倒频谱系数来将音频特征化;输入层的顶部有两层卷积层(时域和频域的二维卷积),然后是三层循环双向GRU,最后是softmax层.卷积层使用单位步长,高度为9(频域),宽度为5(时域)的卷积核.循环层每个方向使用512个单元的GRU.最后一层卷积层和循环层采用概率为0.95的dropout.使用宽度为50的beam search解码计算音素对错误率,约束条件是相邻音素对至少有一个重叠音素,并且跟踪音素的位置;分割模型采用Adam优化,参数设置如下:β1 = 0.9, β2 = 0.999, ε = 10^−8,批数量为128,学习率为10^-4,每500次迭代执行一次概率为0.95的退火;

**3.音素持续时间和基频模型**

音素持续时间和时间相关的基频都由同一架构预测,输入是带重音的一系列音素,音素和重音被编码为one-hot向量.架构包含两个全连接层,每层包含256个单元,然后是两层单向循环层,每层有128个GRU单元,最后是全连接输出层;在初始的全连接层和最后一个循环层使用概率为0.8的dropout.

最后一个全连接层为每个音素产生三个估计值:持续时间,浊音的概率和在预测时间内均匀采样得到的20个基频值.

模型的优化使用如下的损失计算方法:
