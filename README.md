# deep voice论文学习
## 系统结构

![](https://github.com/sysu16340234/deep_voice_learning/blob/master/imgs/deep_voice.png)

系统包含五个基本模块:**字素转换音素模型,分割模型,音素持续时间模型,基频模型,音频合成模型**:

* **字素转换音素模型**:用来将文本转化为音素
* **分割模型**:分割模型用来识别给定音频中每个音素的边界,即开始和结束的位置
* **音素持续时间模型**:用来预测音素序列中每个音素的持续时间
* **基频模型**:预测音素是否为浊音,如果是浊音，则在在音素的持续时间过程中预测基频;
* **音频合成模型**:组合了字素转换音素模型,音素持续时间模型,基频模型,以高采样率生成所需文本的音频;

**训练过程**:给定音频和对应文本,将文本通过G2P模型转换为音素,将音素和音频输入分割模块,使用分割模块将音素的边界作标记即得到音素持续时间,将音素持续时间,音素和音频输入音频合成模型作为训练样本,将持续时间和音素输入持续时间模型作为训练样本,对音频进行基频提取,然后将基频和音素输入基频模型作为训练样本;

**推理过程**:文本经过字素转换音素模型或者使用音素字典转换成音素,再把音素提供给音素持续时间模型和基频模型预测每个音素的持续时间和基频,最后将基频和音素持续时间作为局部条件特征输入音频合成模型,音频合成模型使用这两类特征和音素合成音频;

## 模型细节
**1.字素转换音素模型**

基于[(Yao & Zweig, 2015)](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnnlts.pdf)的编码器和解码器结构,使用具有非线性GRU的多层双向编码器和同样深度的单层GRU解码器;使用teacher forcing,解码采用beam search,使用三层双向1024个单元的解码器和三层单元数相同的解码器,beam search的宽度为5,对于每层采用概率为0.95的dropout.采用Adam优化算法,参数设置:β1 = 0.9, β2 = 0.999, ε = 10^−8,批大小为64,学习率为10^-3,每1000次迭代执行概率为0.85的退火.一个双向LSTMencoder-decoder的例图如下(虚线左边为编码器,右边为解码器):

![encoder_decoder](https://github.com/sysu16340234/deep_voice_learning/blob/master/imgs/encoder_decoder.png)

**2.分割模型**

基于[(Amodei et al., 2015)](https://arxiv.org/pdf/1512.02595.pdf)的卷积递归神经网络实现,在CTC的基础上做出改进:将相邻的两个音素对的边界作为结果输出.通过以10ms为步长计算20个mel倒频谱系数来将音频特征化;输入层的顶部有两层卷积层(时域和频域的二维卷积),然后是三层循环双向GRU,最后是softmax层.卷积层使用单位步长,高度为9(频域),宽度为5(时域)的卷积核.循环层每个方向使用512个单元的GRU.最后一层卷积层和循环层采用概率为0.95的dropout.使用宽度为50的beam search解码计算音素对错误率,约束条件是相邻音素对至少有一个重叠音素,并且跟踪音素的位置;分割模型采用Adam优化,参数设置如下:β1 = 0.9, β2 = 0.999, ε = 10^−8,批数量为128,学习率为10^-4,每500次迭代执行一次概率为0.95的退火;

**3.音素持续时间和基频模型**

音素持续时间和时间相关的基频都由同一架构预测,输入是带重音的一系列音素,音素和重音被编码为one-hot向量.架构包含两个全连接层,每层包含256个单元,然后是两层单向循环层,每层有128个GRU单元,最后是全连接输出层;在初始的全连接层和最后一个循环层使用概率为0.8的dropout.

最后一个全连接层为每个音素产生三个估计值:持续时间,浊音的概率和在预测时间内均匀采样得到的20个基频值.

模型的优化使用如下的损失计算方法:

![loss](https://github.com/sysu16340234/deep_voice_learning/blob/master/imgs/loss.png)

其中λi是常数,^t和t分别是第n个音素持续时间的估计值和实值,^p和p分别是第n个音素是浊音的概率估计值和实值,CE是交叉熵函数,^Fn,t和Fn,t分别是t时刻基频的估计值和实值,T个采样在持续时间内是等间隔的.

**4.音频合成模型**

音频合成模型采用改进的WaveNet,其结构如下:

![model](https://github.com/sysu16340234/deep_voice_learning/blob/master/imgs/wavenet.png)

WaveNet使用一个条件网络c = C(v),将低频语言特征转化为原生音频,以及一个自回归过程P(yi|c, yi−1, . . . , yi−R)在给定当前时间步长和R个音频样本的条件下来预测下一个音频样本,R是感受野,是由网络结构确定的.

* **自回归的WaveNet**

自回归网络结构通过网络的层数l,skip通道数s来和残差通道数r来参数化,使用μ律将音频量化为a = 256个值,one-hot编码在2x1卷积后,为残差堆栈的第一层生成输入x(0):

![](https://github.com/sysu16340234/deep_voice_learning/blob/master/imgs/11.png)

其中*代表卷积运算,y是输入音频的量化值,

在后续的层中,采用以下方法计算隐藏状态的值h(i)和下一个输入x(i):

![](https://github.com/sysu16340234/deep_voice_learning/blob/master/imgs/12.png)

L(i)为该层调整网络的输出,卷积的过程实际上是通过r通道输入2r通道输出的一次卷积完成,而非式中的两次,在推理过程中,这一次卷积会被两个矩阵乘法替代,h(i)的更新如下:

![](https://github.com/sysu16340234/deep_voice_learning/blob/master/imgs/14.png)

式中L(i)是L(i)h和L(i)g的连接,B(i)是B(i)h和B(i)g的连接;

从第1层到第l层的隐藏状态h(i)在经过连接后投影到s个通道:

![](https://github.com/sysu16340234/deep_voice_learning/blob/master/imgs/16.png)

zs最后通过两个全连接层得到输出分布p:

![](https://github.com/sysu16340234/deep_voice_learning/blob/master/imgs/18.png)

* **调整网络**

调整网络的目的是为了产生一组相关的语言特征,交给WaveNet作为调节向量来产生可识别的音频,由于音频的频率远高于语言特征的频率,因此需要对特征进行上采样,上采样的方法是将特征通过两个双向QRNN层将特征传递给fo-pooling层和2x1卷积层,QRNN的具体定义如下:

![](https://github.com/sysu16340234/deep_voice_learning/blob/master/imgs/20.png)

双向QRNN层通过计算输入序列和输入序列的反向副本的两个QRNN实现,将这两个QRNN的输出通道堆叠,在经过两层的QRNN后进行通道交错,使WaveNet中的sigmoid和tanh函数都能得到正向和反向的QRNN输出,最后进行一次重复采样将频率上采样到原生音频的频率
